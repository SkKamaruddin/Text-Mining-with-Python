{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data mining with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Refer: https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "1. Basic feature extraction using text data\n",
    "    * Number of words\n",
    "    * Number of characters\n",
    "    * Average word length\n",
    "    * Number of stopwords\n",
    "    * Number of special characters\n",
    "    * Number of numerics\n",
    "    * Number of uppercase words\n",
    "2. Basic Text Pre-processing of text data\n",
    "    * Lower casing\n",
    "    * Punctuation removal\n",
    "    * Stopwords removal\n",
    "    * Frequent words removal\n",
    "    * Rare words removal\n",
    "    * Spelling correction\n",
    "    * Tokenization\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "3. Advance Text Processing\n",
    "    * N-grams\n",
    "    * Term Frequency\n",
    "    * Inverse Document Frequency\n",
    "    * Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    * Bag of Words\n",
    "    * Sentiment Analysis\n",
    "    * Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let’s quickly read the training file from the dataset in order to perform different tasks on it.\n",
    "we will use the twitter sentiment [dataset](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/ \"Download dataset from here\") from the datahack platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are only working with textual data, but we can also use the below methods when numerical features are also present along with the text.\n",
    "\n",
    "#### 1.1 Number of Words\n",
    "One of the most basic features we can extract is the number of words in each tweet. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones.\n",
    "\n",
    "To do this, we simply use the __split__ function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  word_count\n",
       "0   @user when a father is dysfunctional and is s...          21\n",
       "1  @user @user thanks for #lyft credit i can't us...          22\n",
       "2                                bihday your majesty           5\n",
       "3  #model   i love u take with u all the time in ...          17\n",
       "4             factsguide: society now    #motivation           8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train[['tweet','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  word_count\n",
       "0   1      0   @user when a father is dysfunctional and is s...          21\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...          22\n",
       "2   3      0                                bihday your majesty           5\n",
       "3   4      0  #model   i love u take with u all the time in ...          17\n",
       "4   5      0             factsguide: society now    #motivation           8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Number of characters\n",
    "This feature is also based on the previous feature intuition. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  char_count\n",
       "0   @user when a father is dysfunctional and is s...         102\n",
       "1  @user @user thanks for #lyft credit i can't us...         122\n",
       "2                                bihday your majesty          21\n",
       "3  #model   i love u take with u all the time in ...          86\n",
       "4             factsguide: society now    #motivation          39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['char_count'] = train['tweet'].str.len() ## this also includes spaces\n",
    "train[['tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_woWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>102</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  char_count  \\\n",
       "0   @user when a father is dysfunctional and is s...         102   \n",
       "1  @user @user thanks for #lyft credit i can't us...         122   \n",
       "2                                bihday your majesty          21   \n",
       "3  #model   i love u take with u all the time in ...          86   \n",
       "4             factsguide: society now    #motivation          39   \n",
       "\n",
       "   char_count_woWS  \n",
       "0               82  \n",
       "1              101  \n",
       "2               17  \n",
       "3               70  \n",
       "4               32  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the number of characters excluding the spaces\n",
    "def tweet_length_woWS(tweet):\n",
    "    return tweet.str.len() - tweet.str.count(' ')\n",
    "\n",
    "train['char_count_woWS'] = tweet_length_woWS(train['tweet'])\n",
    "train[['tweet', 'char_count', 'char_count_woWS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_woWS</th>\n",
       "      <th>char_count_WS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>102</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  char_count  \\\n",
       "0   @user when a father is dysfunctional and is s...         102   \n",
       "1  @user @user thanks for #lyft credit i can't us...         122   \n",
       "2                                bihday your majesty          21   \n",
       "3  #model   i love u take with u all the time in ...          86   \n",
       "4             factsguide: society now    #motivation          39   \n",
       "\n",
       "   char_count_woWS  char_count_WS  \n",
       "0               82             82  \n",
       "1              101            101  \n",
       "2               17             17  \n",
       "3               70             70  \n",
       "4               32             32  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the number of characters excluding the spaces\n",
    "# this code is alternative to the above cell where length is calculated by substraction method\n",
    "def tweet_length_WS(tweet):\n",
    "    return len(tweet.replace(\" \", \"\"))\n",
    "\n",
    "train['char_count_WS'] = train['tweet'].apply(lambda x: tweet_length_WS(x))\n",
    "train[['tweet', 'char_count', 'char_count_woWS', 'char_count_WS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_woWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>21</td>\n",
       "      <td>116</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "      <td>23</td>\n",
       "      <td>143</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>â #ireland consumer price index (mom) climb...</td>\n",
       "      <td>21</td>\n",
       "      <td>111</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>we are so selfish. #orlando #standwithorlando ...</td>\n",
       "      <td>16</td>\n",
       "      <td>133</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>i get to see my daddy today!!   #80days #getti...</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
       "      <td>14</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
       "      <td>13</td>\n",
       "      <td>101</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch...junior is angryð#got7 #junior #yugyo...</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for having a paner. #thankful #p...</td>\n",
       "      <td>14</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>its #friday! ð smiles all around via ig use...</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>as we all know, essential oils are not made of...</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>#euro2016 people blaming ha for conceded goal ...</td>\n",
       "      <td>25</td>\n",
       "      <td>127</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>sad little dude..   #badday #coneofshame #cats...</td>\n",
       "      <td>12</td>\n",
       "      <td>70</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>product of the day: happy man #wine tool  who'...</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>@user #tgif   #ff to my #gamedev #indiedev #i...</td>\n",
       "      <td>17</td>\n",
       "      <td>95</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sign by vendor 80 for $45.00!! #upsi...</td>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>@user all #smiles when #media is   !! ðð...</td>\n",
       "      <td>22</td>\n",
       "      <td>127</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>we had a great panel on the mediatization of t...</td>\n",
       "      <td>15</td>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>happy father's day @user ðððð</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50 people went to nightclub to have a good nig...</td>\n",
       "      <td>24</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31932</th>\n",
       "      <td>31933</td>\n",
       "      <td>0</td>\n",
       "      <td>@user thanks gemma</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31933</th>\n",
       "      <td>31934</td>\n",
       "      <td>1</td>\n",
       "      <td>@user judd is a  &amp;amp; #homophobic #freemilo #...</td>\n",
       "      <td>16</td>\n",
       "      <td>108</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31934</th>\n",
       "      <td>31935</td>\n",
       "      <td>1</td>\n",
       "      <td>lady banned from kentucky mall. @user  #jcpenn...</td>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31935</th>\n",
       "      <td>31936</td>\n",
       "      <td>0</td>\n",
       "      <td>ugh i'm trying to enjoy my happy hour drink &amp;a...</td>\n",
       "      <td>29</td>\n",
       "      <td>138</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31936</th>\n",
       "      <td>31937</td>\n",
       "      <td>0</td>\n",
       "      <td>want to know how to live a   life? do more thi...</td>\n",
       "      <td>27</td>\n",
       "      <td>126</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31937</th>\n",
       "      <td>31938</td>\n",
       "      <td>0</td>\n",
       "      <td>love island ð</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31938</th>\n",
       "      <td>31939</td>\n",
       "      <td>0</td>\n",
       "      <td>my fav actor #vijaysethupathi ! my fav actress...</td>\n",
       "      <td>24</td>\n",
       "      <td>112</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31939</th>\n",
       "      <td>31940</td>\n",
       "      <td>0</td>\n",
       "      <td>whew  ð",
       " it's a productive and   #friday!!!</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31940</th>\n",
       "      <td>31941</td>\n",
       "      <td>0</td>\n",
       "      <td>@user she's finally here! @user</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31941</th>\n",
       "      <td>31942</td>\n",
       "      <td>0</td>\n",
       "      <td>passed first year of uni #yay #love #pass #uni...</td>\n",
       "      <td>14</td>\n",
       "      <td>89</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31942</th>\n",
       "      <td>31943</td>\n",
       "      <td>0</td>\n",
       "      <td>this week is flying by   #humpday - #wednesday...</td>\n",
       "      <td>13</td>\n",
       "      <td>62</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31943</th>\n",
       "      <td>31944</td>\n",
       "      <td>0</td>\n",
       "      <td>@user modeling photoshoot this friday yay #mo...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31944</th>\n",
       "      <td>31945</td>\n",
       "      <td>0</td>\n",
       "      <td>you're surrounded by people who love you (even...</td>\n",
       "      <td>21</td>\n",
       "      <td>101</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31945</th>\n",
       "      <td>31946</td>\n",
       "      <td>0</td>\n",
       "      <td>feel like... ðð¶ð #dog #summer #hot #h...</td>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>31947</td>\n",
       "      <td>1</td>\n",
       "      <td>@user omfg i'm offended! i'm a  mailbox and i'...</td>\n",
       "      <td>14</td>\n",
       "      <td>82</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>31948</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user you don't have the balls to hashta...</td>\n",
       "      <td>24</td>\n",
       "      <td>112</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31948</th>\n",
       "      <td>31949</td>\n",
       "      <td>1</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "      <td>19</td>\n",
       "      <td>87</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31949</th>\n",
       "      <td>31950</td>\n",
       "      <td>0</td>\n",
       "      <td>hear one of my new songs! don't go - katie ell...</td>\n",
       "      <td>21</td>\n",
       "      <td>110</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31950</th>\n",
       "      <td>31951</td>\n",
       "      <td>0</td>\n",
       "      <td>@user you can try to 'tail' us to stop, 'butt...</td>\n",
       "      <td>25</td>\n",
       "      <td>115</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31951</th>\n",
       "      <td>31952</td>\n",
       "      <td>0</td>\n",
       "      <td>i've just posted a new blog: #secondlife #lone...</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31952</th>\n",
       "      <td>31953</td>\n",
       "      <td>0</td>\n",
       "      <td>@user you went too far with @user</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31953</th>\n",
       "      <td>31954</td>\n",
       "      <td>0</td>\n",
       "      <td>good morning #instagram #shower #water #berlin...</td>\n",
       "      <td>15</td>\n",
       "      <td>101</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>31955</td>\n",
       "      <td>0</td>\n",
       "      <td>#holiday   bull up: you will dominate your bul...</td>\n",
       "      <td>24</td>\n",
       "      <td>108</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31955</th>\n",
       "      <td>31956</td>\n",
       "      <td>0</td>\n",
       "      <td>less than 2 weeks ð",
       "ðð¼ð¹ððµ @us...</td>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31956</th>\n",
       "      <td>31957</td>\n",
       "      <td>0</td>\n",
       "      <td>off fishing tomorrow @user carnt wait first ti...</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>25</td>\n",
       "      <td>131</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>15</td>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1      0   @user when a father is dysfunctional and is s...   \n",
       "1          2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3      0                                bihday your majesty   \n",
       "3          4      0  #model   i love u take with u all the time in ...   \n",
       "4          5      0             factsguide: society now    #motivation   \n",
       "5          6      0  [2/2] huge fan fare and big talking before the...   \n",
       "6          7      0   @user camping tomorrow @user @user @user @use...   \n",
       "7          8      0  the next school year is the year for exams.ð...   \n",
       "8          9      0  we won!!! love the land!!! #allin #cavs #champ...   \n",
       "9         10      0   @user @user welcome here !  i'm   it's so #gr...   \n",
       "10        11      0   â #ireland consumer price index (mom) climb...   \n",
       "11        12      0  we are so selfish. #orlando #standwithorlando ...   \n",
       "12        13      0  i get to see my daddy today!!   #80days #getti...   \n",
       "13        14      1  @user #cnn calls #michigan middle school 'buil...   \n",
       "14        15      1  no comment!  in #australia   #opkillingbay #se...   \n",
       "15        16      0  ouch...junior is angryð#got7 #junior #yugyo...   \n",
       "16        17      0  i am thankful for having a paner. #thankful #p...   \n",
       "17        18      1                             retweet if you agree!    \n",
       "18        19      0  its #friday! ð smiles all around via ig use...   \n",
       "19        20      0  as we all know, essential oils are not made of...   \n",
       "20        21      0  #euro2016 people blaming ha for conceded goal ...   \n",
       "21        22      0  sad little dude..   #badday #coneofshame #cats...   \n",
       "22        23      0  product of the day: happy man #wine tool  who'...   \n",
       "23        24      1    @user @user lumpy says i am a . prove it lumpy.   \n",
       "24        25      0   @user #tgif   #ff to my #gamedev #indiedev #i...   \n",
       "25        26      0  beautiful sign by vendor 80 for $45.00!! #upsi...   \n",
       "26        27      0   @user all #smiles when #media is   !! ðð...   \n",
       "27        28      0  we had a great panel on the mediatization of t...   \n",
       "28        29      0        happy father's day @user ðððð     \n",
       "29        30      0  50 people went to nightclub to have a good nig...   \n",
       "...      ...    ...                                                ...   \n",
       "31932  31933      0                               @user thanks gemma     \n",
       "31933  31934      1  @user judd is a  &amp; #homophobic #freemilo #...   \n",
       "31934  31935      1  lady banned from kentucky mall. @user  #jcpenn...   \n",
       "31935  31936      0  ugh i'm trying to enjoy my happy hour drink &a...   \n",
       "31936  31937      0  want to know how to live a   life? do more thi...   \n",
       "31937  31938      0                                 love island ð     \n",
       "31938  31939      0  my fav actor #vijaysethupathi ! my fav actress...   \n",
       "31939  31940      0      whew  ð\n",
       " it's a productive and   #friday!!!   \n",
       "31940  31941      0                 @user she's finally here! @user      \n",
       "31941  31942      0  passed first year of uni #yay #love #pass #uni...   \n",
       "31942  31943      0  this week is flying by   #humpday - #wednesday...   \n",
       "31943  31944      0   @user modeling photoshoot this friday yay #mo...   \n",
       "31944  31945      0  you're surrounded by people who love you (even...   \n",
       "31945  31946      0  feel like... ðð¶ð #dog #summer #hot #h...   \n",
       "31946  31947      1  @user omfg i'm offended! i'm a  mailbox and i'...   \n",
       "31947  31948      1  @user @user you don't have the balls to hashta...   \n",
       "31948  31949      1   makes you ask yourself, who am i? then am i a...   \n",
       "31949  31950      0  hear one of my new songs! don't go - katie ell...   \n",
       "31950  31951      0   @user you can try to 'tail' us to stop, 'butt...   \n",
       "31951  31952      0  i've just posted a new blog: #secondlife #lone...   \n",
       "31952  31953      0                @user you went too far with @user     \n",
       "31953  31954      0  good morning #instagram #shower #water #berlin...   \n",
       "31954  31955      0  #holiday   bull up: you will dominate your bul...   \n",
       "31955  31956      0  less than 2 weeks ð\n",
       "ðð¼ð¹ððµ @us...   \n",
       "31956  31957      0  off fishing tomorrow @user carnt wait first ti...   \n",
       "31957  31958      0  ate @user isz that youuu?ðððððð...   \n",
       "31958  31959      0    to see nina turner on the airwaves trying to...   \n",
       "31959  31960      0  listening to sad songs on a monday morning otw...   \n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...   \n",
       "31961  31962      0                   thank you @user for you follow     \n",
       "\n",
       "       word_count  char_count  char_count_woWS  \n",
       "0              21         102               82  \n",
       "1              22         122              101  \n",
       "2               5          21               17  \n",
       "3              17          86               70  \n",
       "4               8          39               32  \n",
       "5              21         116               96  \n",
       "6              12          74               63  \n",
       "7              23         143              121  \n",
       "8              13          87               75  \n",
       "9              15          50               36  \n",
       "10             21         111               91  \n",
       "11             16         133              118  \n",
       "12             11          51               41  \n",
       "13             14          74               61  \n",
       "14             13         101               89  \n",
       "15              9          56               48  \n",
       "16             14          58               45  \n",
       "17              5          22               18  \n",
       "18             16          78               63  \n",
       "19             12          58               47  \n",
       "20             25         127              103  \n",
       "21             12          70               59  \n",
       "22             22         100               79  \n",
       "23             11          47               37  \n",
       "24             17          95               79  \n",
       "25             13          79               67  \n",
       "26             22         127              106  \n",
       "27             15          72               58  \n",
       "28              7          43               37  \n",
       "29             24         135              112  \n",
       "...           ...         ...              ...  \n",
       "31932           5          20               16  \n",
       "31933          16         108               93  \n",
       "31934          11          59               49  \n",
       "31935          29         138              110  \n",
       "31936          27         126              100  \n",
       "31937           5          18               14  \n",
       "31938          24         112               89  \n",
       "31939          10          45               36  \n",
       "31940           8          34               27  \n",
       "31941          14          89               76  \n",
       "31942          13          62               50  \n",
       "31943          14          69               56  \n",
       "31944          21         101               81  \n",
       "31945          13          68               56  \n",
       "31946          14          82               69  \n",
       "31947          24         112               89  \n",
       "31948          19          87               69  \n",
       "31949          21         110               90  \n",
       "31950          25         115               91  \n",
       "31951          12          57               46  \n",
       "31952           9          35               27  \n",
       "31953          15         101               87  \n",
       "31954          24         108               85  \n",
       "31955           9          92               84  \n",
       "31956          13          61               49  \n",
       "31957           6          68               63  \n",
       "31958          25         131              107  \n",
       "31959          15          63               49  \n",
       "31960          13          67               55  \n",
       "31961           8          32               25  \n",
       "\n",
       "[31962 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for dropping the column 'char_count_WS' from the pandas dataframe\n",
    "\n",
    "train.drop(['char_count_WS'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Average Word Length\n",
    "We will also extract another feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>5.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>4.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  avg_word\n",
       "0   @user when a father is dysfunctional and is s...  4.555556\n",
       "1  @user @user thanks for #lyft credit i can't us...  5.315789\n",
       "2                                bihday your majesty  5.666667\n",
       "3  #model   i love u take with u all the time in ...  4.928571\n",
       "4             factsguide: society now    #motivation  8.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['tweet'].apply(lambda x: avg_word(x))\n",
    "train[['tweet','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Number of stopwords\n",
    "Generally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n",
    "\n",
    "Here, we have imported stopwords from NLTK, which is a basic NLP library in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't run it. Already executed\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  stopwords\n",
       "0   @user when a father is dysfunctional and is s...         10\n",
       "1  @user @user thanks for #lyft credit i can't us...          5\n",
       "2                                bihday your majesty          1\n",
       "3  #model   i love u take with u all the time in ...          5\n",
       "4             factsguide: society now    #motivation          1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['stopwords'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train[['tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Number of special characters\n",
    "One more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n",
    "\n",
    "Here, we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  hastags\n",
       "0   @user when a father is dysfunctional and is s...        1\n",
       "1  @user @user thanks for #lyft credit i can't us...        3\n",
       "2                                bihday your majesty        0\n",
       "3  #model   i love u take with u all the time in ...        1\n",
       "4             factsguide: society now    #motivation        1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hastags'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "train[['tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Number of numerics\n",
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  numerics\n",
       "0   @user when a father is dysfunctional and is s...         0\n",
       "1  @user @user thanks for #lyft credit i can't us...         0\n",
       "2                                bihday your majesty         0\n",
       "3  #model   i love u take with u all the time in ...         0\n",
       "4             factsguide: society now    #motivation         0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['numerics'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "train[['tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Number of Uppercase words\n",
    "Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  upper\n",
       "0   @user when a father is dysfunctional and is s...      0\n",
       "1  @user @user thanks for #lyft credit i can't us...      0\n",
       "2                                bihday your majesty      0\n",
       "3  #model   i love u take with u all the time in ...      0\n",
       "4             factsguide: society now    #motivation      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['upper'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "train[['tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Pre-processing\n",
    "So far, we have learned how to extract basic features from text data. Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Lower case\n",
    "The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @user when a father is dysfunctional and is so...\n",
       "1    @user @user thanks for #lyft credit i can't us...\n",
       "2                                  bihday your majesty\n",
       "3    #model i love u take with u all the time in ur...\n",
       "4                  factsguide: society now #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Removing Punctuation\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user when a father is dysfunctional and is so ...\n",
       "1    user user thanks for lyft credit i cant use ca...\n",
       "2                                  bihday your majesty\n",
       "3    model i love u take with u all the time in urð...\n",
       "4                    factsguide society now motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above output, all the punctuation, including ‘#’ and ‘@’, has been removed from the training data.\n",
    "\n",
    " \n",
    "\n",
    "#### 2.3 Removal of Stop Words\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user father dysfunctional selfish drags kids d...\n",
       "1    user user thanks lyft credit cant use cause do...\n",
       "2                                       bihday majesty\n",
       "3                model love u take u time urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Common word removal\n",
    "Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "life        1086\n",
       "like        1042\n",
       "today        991\n",
       "new          983\n",
       "positive     928\n",
       "thankful     919\n",
       "get          917\n",
       "people       852\n",
       "good         840\n",
       "bihday       825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s remove these words as their presence will not of any use in classification of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Rare words removal\n",
    "Similarly, just as we removed the most common words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "âpinkãpinkâ            1\n",
       "laconza_williams_sr    1\n",
       "drew                   1\n",
       "bestabiamo             1\n",
       "peâ                    1\n",
       "630am                  1\n",
       "tashaneishqâ           1\n",
       "topeka                 1\n",
       "luckiest               1\n",
       "untukâ                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kids dysfun...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective.\n",
    "\n",
    " \n",
    "\n",
    "#### 2.6 Spelling correction\n",
    "We’ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.\n",
    "\n",
    "In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous [article](https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/ \"NLP with textblob\") on ‘NLP for beginners using textblob’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kiss dysfun...\n",
       "1    thanks left credit can use cause dont offer wh...\n",
       "2                                       midday majesty\n",
       "3                               model take or ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it will actually take a lot of time to make these corrections. Therefore, just for the purposes of learning, I have shown this technique by applying it on only the first 5 rows. Moreover, we cannot always expect it to be accurate so some care should be taken before applying it.\n",
    "\n",
    "We should also keep in mind that words are often used in their abbreviated form. For instance, ‘your’ is used as ‘ur’. We should treat this before the spelling correction step, otherwise these words might be transformed into any other word like the one shown below:\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Tokenization\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**********************************************************************\n",
    "  Resource punkt not found.\n",
    "  Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('punkt')\n",
    "  \n",
    "  For more information see: https://www.nltk.org/data.html\n",
    "\n",
    "  Attempted to load tokenizers/punkt/PY3/english.pickle\n",
    "\n",
    "  Searched in:\n",
    "    - '/home/hduser1/nltk_data'\n",
    "    - '/usr/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/local/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/local/lib/nltk_data'\n",
    "    - ''\n",
    "**********************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser1/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['thanks', 'lyft', 'credit', 'cant', 'use', 'cause', 'dont', 'offer', 'wheelchair', 'vans', 'pdx', 'disapointed', 'getthanked'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(train['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Stemming\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        father dysfunct selfish drag kid dysfunct run\n",
       "1    thank lyft credit cant use caus dont offer whe...\n",
       "2                                       bihday majesti\n",
       "3                              model take urð ðððð ððð\n",
       "4                              factsguid societi motiv\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, dysfunctional has been transformed into dysfunct, among other changes.\n",
    "\n",
    " \n",
    "\n",
    "#### 2.9 Lemmatization\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**********************************************************************\n",
    "  Resource wordnet not found.\n",
    "  Please use the NLTK Downloader to obtain the resource:\n",
    "\n",
    "  >>> import nltk\n",
    "  >>> nltk.download('wordnet')\n",
    "  \n",
    "  For more information see: https://www.nltk.org/data.html\n",
    "\n",
    "  Attempted to load corpora/wordnet\n",
    "\n",
    "  Searched in:\n",
    "    - '/home/hduser1/nltk_data'\n",
    "    - '/usr/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/local/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/local/lib/nltk_data'\n",
    "**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hduser1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drag kid dysfunct...\n",
       "1    thanks lyft credit cant use cause dont offer w...\n",
       "2                                       bihday majesty\n",
       "3                              model take urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Advance Text Processing\n",
    "Up to this point, we have done all the basic pre-processing steps in order to clean our data. Now, we can finally move on to extracting features using NLP techniques.\n",
    "\n",
    " \n",
    "\n",
    "#### 3.1 N-grams\n",
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "So, let’s quickly extract bigrams from our tweets using the ngrams function of the textblob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['father', 'dysfunctional']),\n",
       " WordList(['dysfunctional', 'selfish']),\n",
       " WordList(['selfish', 'drag']),\n",
       " WordList(['drag', 'kid']),\n",
       " WordList(['kid', 'dysfunction']),\n",
       " WordList(['dysfunction', 'run'])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(train['tweet'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Term frequency\n",
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "Therefore, we can generalize term frequency as:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "To understand more about Term Frequency, have a look at [this article](https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/).\n",
    "\n",
    "Below, I have tried to show you the term frequency table of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pdx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  tf\n",
       "0         offer   1\n",
       "1    wheelchair   1\n",
       "2   disapointed   1\n",
       "3         cause   1\n",
       "4          dont   1\n",
       "5    getthanked   1\n",
       "6        credit   1\n",
       "7           van   1\n",
       "8           pdx   1\n",
       "9        thanks   1\n",
       "10          use   1\n",
       "11         cant   1\n",
       "12         lyft   1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about term frequency in this [article](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Inverse Document Frequency\n",
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "\n",
    "So, let’s calculate IDF for the same tweets for which we calculated the term frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "      <td>6.522155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>1</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>5.690172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont</td>\n",
       "      <td>1</td>\n",
       "      <td>3.745585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>1</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "      <td>7.327781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "      <td>5.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pdx</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "      <td>4.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "      <td>3.552287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "      <td>3.538194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  tf        idf\n",
       "0         offer   1   6.522155\n",
       "1    wheelchair   1   9.273691\n",
       "2   disapointed   1  10.372303\n",
       "3         cause   1   5.690172\n",
       "4          dont   1   3.745585\n",
       "5    getthanked   1   9.679156\n",
       "6        credit   1   7.327781\n",
       "7           van   1   5.236505\n",
       "8           pdx   1   8.762865\n",
       "9        thanks   1   4.597751\n",
       "10          use   1   3.552287\n",
       "11         cant   1   3.538194\n",
       "12         lyft   1   8.762865"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the value of IDF, the more unique is the word.\n",
    "\n",
    " \n",
    "\n",
    "#### 3.4 Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "TF-IDF is the multiplication of the TF and IDF which we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "      <td>6.522155</td>\n",
       "      <td>6.522155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>1</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>5.690172</td>\n",
       "      <td>5.690172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont</td>\n",
       "      <td>1</td>\n",
       "      <td>3.745585</td>\n",
       "      <td>3.745585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>1</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "      <td>7.327781</td>\n",
       "      <td>7.327781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "      <td>5.236505</td>\n",
       "      <td>5.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pdx</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "      <td>4.597751</td>\n",
       "      <td>4.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "      <td>3.552287</td>\n",
       "      <td>3.552287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "      <td>3.538194</td>\n",
       "      <td>3.538194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  tf        idf      tfidf\n",
       "0         offer   1   6.522155   6.522155\n",
       "1    wheelchair   1   9.273691   9.273691\n",
       "2   disapointed   1  10.372303  10.372303\n",
       "3         cause   1   5.690172   5.690172\n",
       "4          dont   1   3.745585   3.745585\n",
       "5    getthanked   1   9.679156   9.679156\n",
       "6        credit   1   7.327781   7.327781\n",
       "7           van   1   5.236505   5.236505\n",
       "8           pdx   1   8.762865   8.762865\n",
       "9        thanks   1   4.597751   4.597751\n",
       "10          use   1   3.552287   3.552287\n",
       "11         cant   1   3.538194   3.538194\n",
       "12         lyft   1   8.762865   8.762865"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.\n",
    "\n",
    "We don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114048 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform basic pre-processing steps like lower-casing and removal of stopwords, if we haven’t done them earlier.\n",
    "\n",
    " \n",
    "\n",
    "#### 3.5 Bag of Words\n",
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n",
    "\n",
    "For implementation, sklearn provides a separate function for it as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128385 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a better understanding of this, you can refer to this [article](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/).\n",
    "\n",
    " \n",
    "\n",
    "#### 3.6 Sentiment Analysis\n",
    "If you recall, our problem was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library), let’s check the sentiment of the first few tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (-0.3, 0.5354166666666667)\n",
       "1                    (0.2, 0.2)\n",
       "2                    (0.0, 0.0)\n",
       "3                    (0.0, 0.0)\n",
       "4                    (0.0, 0.0)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks lyft credit cant use cause dont offer w...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model take urð ðððð ððð</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  father dysfunctional selfish drag kid dysfunct...       -0.3\n",
       "1  thanks lyft credit cant use cause dont offer w...        0.2\n",
       "2                                     bihday majesty        0.0\n",
       "3                            model take urð ðððð ððð        0.0\n",
       "4                      factsguide society motivation        0.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "train[['tweet','sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Word Embeddings\n",
    "Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors.\n",
    "\n",
    "Word2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki, etc.\n",
    "\n",
    "Here, we will use pre-trained word vectors which can be downloaded from the [glove](https://nlp.stanford.edu/projects/glove/) website. There are different dimensions (50,100, 200, 300) vectors trained on wiki data. For this example, I have downloaded the 100-dimensional version of the model.\n",
    "\n",
    "You can refer an article [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) to understand different form of word embeddings.\n",
    "\n",
    "The first step here is to convert it into the word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the above word2vec file as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say our tweet contains a text saying ‘go away’. We can easily obtain it’s word vector using the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.078894  ,  0.46160001,  0.57779002, -0.71636999, -0.13121   ,\n",
       "        0.41859999, -0.29155999,  0.52006   ,  0.089986  , -0.35062   ,\n",
       "        0.51754999,  0.51998001,  0.15218   ,  0.41485   , -0.12377   ,\n",
       "       -0.37222001,  0.0273    ,  0.75673002, -0.8739    ,  0.58934999,\n",
       "        0.46662   ,  0.62918001,  0.092603  , -0.012868  , -0.015169  ,\n",
       "        0.25567001, -0.43024999, -0.77667999,  0.71449   , -0.38339999,\n",
       "       -0.69638002,  0.23522   ,  0.11396   ,  0.02778   ,  0.071357  ,\n",
       "        0.87409002, -0.12809999,  0.063576  ,  0.067867  , -0.50181001,\n",
       "       -0.28523001, -0.072536  , -0.50738001, -0.69139999, -0.53579003,\n",
       "       -0.11361   , -0.38234001, -0.12414   ,  0.011214  , -1.16219997,\n",
       "        0.037057  , -0.18494999,  0.01416   ,  0.87193   , -0.097309  ,\n",
       "       -2.35649991, -0.14554   ,  0.28275001,  2.00530005,  0.23439001,\n",
       "       -0.38297999,  0.69538999, -0.44916001, -0.094157  ,  0.90526998,\n",
       "        0.65763998,  0.27627999,  0.30688   , -0.57780999, -0.22987001,\n",
       "       -0.083043  , -0.57235998, -0.29899999, -0.81111997,  0.039752  ,\n",
       "       -0.05681   , -0.48879001, -0.18091001, -0.28152001, -0.20558999,\n",
       "        0.4932    , -0.033999  , -0.53139001, -0.28297001, -1.44749999,\n",
       "       -0.18685   ,  0.091177  ,  0.11454   , -0.28167999, -0.33565   ,\n",
       "       -0.31663001, -0.1089    ,  0.10111   , -0.23737   , -0.64955002,\n",
       "       -0.26800001,  0.35095999,  0.26352   ,  0.59397   ,  0.26741001], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10379   , -0.014792  ,  0.59933001, -0.51315999, -0.036463  ,\n",
       "        0.65880001, -0.57906002,  0.17818999,  0.23662999, -0.21383999,\n",
       "        0.55339003,  0.53596997,  0.041444  ,  0.16095001,  0.017093  ,\n",
       "       -0.37242001,  0.017974  ,  0.39267999, -0.23265   ,  0.18179999,\n",
       "        0.66404998,  0.98163003,  0.42339   ,  0.030581  ,  0.35014999,\n",
       "        0.25519001, -0.71182001, -0.42184001,  0.13067999, -0.47452   ,\n",
       "       -0.08175   ,  0.1574    , -0.13262001,  0.22679   , -0.16885   ,\n",
       "       -0.11122   , -0.32271999, -0.020978  , -0.43345001,  0.17200001,\n",
       "       -0.67365998, -0.79052001,  0.10556   , -0.4219    , -0.12385   ,\n",
       "       -0.063486  , -0.17843001,  0.56358999,  0.16986001, -0.17804   ,\n",
       "        0.13956   , -0.20169   ,  0.078985  ,  1.4497    ,  0.23556   ,\n",
       "       -2.6013999 , -0.52859998, -0.11636   ,  1.7184    ,  0.33254001,\n",
       "        0.12136   ,  1.1602    , -0.29139999,  0.47125   ,  0.41869   ,\n",
       "        0.35271001,  0.47869   , -0.042281  , -0.18294001,  0.1796    ,\n",
       "       -0.24431001, -0.34042001,  0.20337   , -0.93676001,  0.013077  ,\n",
       "        0.080339  , -0.36603999, -0.44005001, -0.35393   ,  0.15907   ,\n",
       "        0.55807   ,  0.14920001, -0.86432999,  0.040305  , -1.09389997,\n",
       "       -0.26385999, -0.29493999,  0.25696   , -0.33717999, -0.086468  ,\n",
       "       -0.24246   , -0.21114001,  0.099632  ,  0.12815   , -0.78714001,\n",
       "       -0.51784998, -0.10944   ,  0.9763    ,  0.57032001,  0.13581   ], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the average to represent the string ‘go away’ in the form of vectors having 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.091342  ,  0.22340401,  0.58855999, -0.61476499, -0.0838365 ,\n",
       "        0.53869998, -0.43531001,  0.349125  ,  0.16330799, -0.28222999,\n",
       "        0.53547001,  0.52797496,  0.096812  ,  0.2879    , -0.0533385 ,\n",
       "       -0.37232   ,  0.022637  ,  0.574705  , -0.55327499,  0.385575  ,\n",
       "        0.56533498,  0.80540502,  0.2579965 ,  0.0088565 ,  0.1674905 ,\n",
       "        0.25543001, -0.57103503, -0.59925997,  0.42258501, -0.42896   ,\n",
       "       -0.389065  ,  0.19631   , -0.00933   ,  0.127285  , -0.0487465 ,\n",
       "        0.38143501, -0.22540998,  0.021299  , -0.1827915 , -0.16490501,\n",
       "       -0.47944498, -0.431528  , -0.20091   , -0.55664998, -0.32982001,\n",
       "       -0.088548  , -0.28038502,  0.219725  ,  0.090537  , -0.67012   ,\n",
       "        0.0883085 , -0.19332001,  0.0465725 ,  1.160815  ,  0.0691255 ,\n",
       "       -2.47895002, -0.33706999,  0.083195  ,  1.86185002,  0.283465  ,\n",
       "       -0.13080999,  0.92779499, -0.37028   ,  0.18854649,  0.66197997,\n",
       "        0.50517499,  0.37748498,  0.1322995 , -0.380375  , -0.025135  ,\n",
       "       -0.1636765 , -0.45638999, -0.047815  , -0.87393999,  0.0264145 ,\n",
       "        0.0117645 , -0.42741501, -0.31048   , -0.317725  , -0.02326   ,\n",
       "        0.525635  ,  0.05760051, -0.69786   , -0.1213325 , -1.27069998,\n",
       "       -0.225355  , -0.1018815 ,  0.18575001, -0.30943   , -0.211059  ,\n",
       "       -0.27954501, -0.16002001,  0.100371  , -0.05461   , -0.71834505,\n",
       "       -0.39292499,  0.12075999,  0.61991   ,  0.58214498,  0.20161   ], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model['go'] + model['away'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted the entire string into a vector which can now be used as a feature in any modelling technique.\n",
    "\n",
    " \n",
    "\n",
    "## End Notes\n",
    "I hope that now you have a basic understanding of how to deal with text data in predictive modeling. These methods will help in extracting more information which in return will help you in building better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
